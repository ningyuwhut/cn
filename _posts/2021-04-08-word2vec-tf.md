---
  layout: post
  title: word2vec的tf实现
  categories: MachineLearning
  tags:
--- 


nce loss ：
https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/nn_impl.py

nce loss 也是为了解决输出层使用softmax计算所有候选word的概率时效率过低的问题。他把基于softmax的一个超大规模的分类问题转换为少量类别的二分类。具体来说，对于一个窗口中的某个中心词,其中的context word为正样本，全局随机负采样几个word作为负样本。然后分别计算正样本和负采样出来的几个负样本为正样本的概率,并计算交叉熵损失。这就是NCE Loss了。

tensorflow中还有一种实现，就是sampled_softmax_loss。这个和nce loss的区别在于这个不再是二分类了，而是一个类别数目变小了的softmax。


这两个损失都只在训练时这么用，在测试和评估时计算完整的sigmoid交叉熵或者softmax损失。

因为这两个损失是为了加速训练，减少计算量，在评估或者测试时我们还是需要计算所有候选类别的概率，在所有类别中选择最优的。
