---
  layout: post
  title: batch normalization 推导
  categories: MachineLearning
  tags:
---

1.反向推导

bn层在神经网络中是放在relu层前面的,dropout层是放在relu后面的



1.http://cthorey.github.io/backpropagation/
2.https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
