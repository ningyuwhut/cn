---
  layout: post
  title: SVD 和PCA
  categories: MachineLearning
  tags:
--- 

PCA变换是一种常见的降维方法，该方法将原始矩阵X经过线性变换（Y=PX)转换到新的空间上，在新的空间上的表示矩阵Y的维度比X更低，且最大限度的保留X中的信息。那么怎么才能既能降维又能最大程度的保留信息呢？我们可以从下面两个角度考虑：

  1.如果新的表示Y的每个维度上的方差越大，那么在该维度上的信息就越多（方差越大越分散，熵越大，信息越多）；
  2.不同维度之间是线性无关的，只有这样，才能保证不同维度之间的信息没有重合。

根据以上两点，我们可以联系到Y的协方差矩阵(协方差的含义)。如果X已经归一化了（均值为0，方差为1），那么为了使得信息最大化，那么Y的协方差矩阵就需要是一个对角阵。因为不同维度之间线性无关，所以协方差为0，对应到协方差矩阵上就是所有非对角线上的元素为0。这样就满足上面的第2点了。对于第1点，我们需要使得协方差矩阵上的元素和越大越好。越大则保留的信息越多。那么我们就面临一个问题，怎么才能使得Y的协方差矩阵是一个对角阵且对角阵上的元素之和越大？

假设X的协方差矩阵为C，Y的协方差矩阵为D,那么可以经过推导，对矩阵C进行对角化就可以使D对角化了。而让C成为对角阵，只需要C的特征向量和特征值即可。（CE=EΛ)

先插入一段基础知识，在Y中，方差最大的变量称之为第一主成分，第二大的称为第二主成分，依次类推。由于C 是一个对称矩阵，且是一个方阵，而对于正定对称方阵，其可以进行特征值分解:

$$
C=Q\Sigma Q^{-1}
$$

Q是C的特征向量组成的矩阵，$\Sigma$是对角阵，对角线上的元素就是特征值，且从小到大排列。

对于一个矩阵A来说，它的协方差矩阵是:

$$
A^TA
$$

使用拉格朗日乘子法，可以得出，Y的第k行是X的第k个主成分，其系数向量就是C的第k个特征向量，对应的特征值是第k个主成分的方差。所以，将C的特征向量按行进行排列，且按照特征值的大小顺序来排列特征向量，假设这个矩阵为P，那么P的前K行构成的矩阵再乘以X，得到的就是Y降维后的表示。

奇异值分解和PCA之间的关系是什么呢？


PCA 除了通过上面的协方差矩阵对角化进行求解外。还可以通过SVD来进行求解。

假设矩阵A的奇异值分解为

$$
A=U\sum V^T
$$

那么有:

$$
A^TA=(U\sum V^T)^T(U\sum V^T)=V(\sum^T\sum)V^T
$$

可以看到，V的列向量是A的协方差矩阵的特征向量，而V是矩阵A的右奇异矩阵。

而根据这个关系，我们可以通过求解A的奇异值分解来对矩阵A进行PCA降维。

特征分解是针对方阵的，而SVD分解是针对任意实数矩阵的。

SVD其实也是可以通过特征值分解来求解的。

假设矩阵A为(m,n)阶，那么$$AA^T$$或者$$A^TA$$就是方阵。这样，就可以对他们进行特征分解了。





参考：

http://blog.codinglabs.org/articles/pca-tutorial.html


https://www.cnblogs.com/pinard/p/6251584.html


https://www.chamwen.com/2018/08/12/ML_covmatrix/


https://njuferret.github.io/2019/07/28/2019-07-28_geometric-interpretation-covariance-matrix/



