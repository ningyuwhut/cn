---
layout: post 
title: 潜在语义分析(Latent Semantic Analysis)
categories:
- Machine Learning
tags:
- machine learning
---

潜在语义分析

潜在语义分析又叫潜在语义索引(latent semantic analysis),意在分析文档背后隐含的语义概念.该问题源于如何找到和查询相关的文档.
潜在语义分析的基本方法是把单词和文档都映射到一个语义空间中并这这个语义空间中比较.

因为文档中的单词其实是可以变化的,这样就会对分析单词和文档之间的关系带来噪声.为了解决这个问题,潜在语义分析引入如下假设:

1. 文档使用 bags of words模型进行表示,该模型忽略单词之间的顺序,只统计每个单词在文档中出现的次数.

2. 概念(concept)由文档中经常一起出现的单词来表示.

3.假设单词只有一种含义.


SVD可以对矩阵进行降维,留下主要成分,去除噪声.

SVD中的一个技巧就是留下多少奇异值,太少会损失信息,太多又会引入噪声.

SVD把一个矩阵A分解为3个矩阵.

    A= USV^T

其中,U给出语义空间中每个单词的坐标,V^T给出每个文档在语义空间中的坐标.S包含留下的奇异值.

为了选择正确的维度,我们可以做一个奇异值平方值的直方图.该图给出了每个奇异值对近似矩阵的贡献.

没有对矩阵进行归一化(centered)是因为这样会把一个稀疏矩阵编程稠密矩阵,会显著增加内存和计算成本.

LSA的优点:

1.把文档和单词映射到同一个语义空间,在这个空间上我们可以对单词和文档进行聚类,更重要的,我们可以根据两种聚类结果的重合程度来从单词检索文档或者从文档检索单词.

2.语义空间维度更少,而且以最少的维度包含了最多的信息.这样这个空间就很适合检测各种聚类算法.

3.LSA是全局算法,它从所有文档和所有单词出发发现其中的模式,所以它可以发现在局部算法看来不明显的性质.它也可以和局部算法(最近邻算法)组合变成更强大的算法.

缺点:

1.LSA假设Gausssian 分布和 Frobenius norm,不是所有问题都有这种性质,比如,文档中的单词符合泊松分布而不是高斯分布

2.不可以处理多义词.它假设每个单词只有一种含义.

3.LSA依赖SVD而SVD计算复杂,在新文档出现时难以更新.

求解SVD的过程:

1.求解矩阵AA^T的特征值和特征向量.
2.奇异值就是特征值的平方根
3.奇异值按降序组成矩阵S的对角元素.
4.每个特征向量除以各自的长度后得到矩阵U的列.
5.相应地,矩阵A^TA的特征向量组成矩阵V的列.

相似度:

1.通过计算U*S矩阵的行与行之间的余弦距离得到对应的单词的相似度.
2.通过计算S*V^T的列与列之间的相似度得到对应的文档的相似度.
3.通过计算U*S^(1/2)的行向量和S^(1/2)*V^T的列向量的余弦距离得到单词和文档之间的相似度.

查询:

首先把查询表示成一个向量.
然后通过如下变换构造出一个pseudo-document.

    Q = q^t*U*S-1

然后就可以使用上面提到的相似度计算方法计算查询文档和其他文档或单词的相似度.
参考:

1.http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html
2.http://www.slideshare.net/sudarsun/latent-semantic-indexing-for-information-retrieval
